{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a621d9-e3b1-459a-a39f-1a9724d26638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import prophet\n",
    "    print(\"Prophet already installed.\")\n",
    "except ImportError:\n",
    "    print(\"Prophet not found. Installing...\")\n",
    "    %pip install prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96ee77fe-1635-42d0-9989-b4f987c5604d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION & IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from prophet import Prophet\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from prophet.plot import plot_plotly, plot_components_plotly\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"All libraries imported successfully\")\n",
    "print(\"Configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd71fe37-f2ed-4492-8481-cacc8ab21d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 1. Data Loading & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef1b5103-b39b-4a8e-a362-245a2fc754e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA LOADING\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Campaign Related\n",
    "campaign_desc = spark.table(\"campaign_desc\")\n",
    "campaign_table = spark.table(\"campaign_table\")\n",
    "\n",
    "# Coupon Related\n",
    "coupon_redempt = spark.table(\"coupon_redempt\")\n",
    "\n",
    "# Transaction Related\n",
    "transaction_data = spark.table(\"transaction_data\")\n",
    "\n",
    "print(f\"Campaign Descriptions: {campaign_desc.count():,} rows\")\n",
    "print(f\"Campaign Table: {campaign_table.count():,} rows\")\n",
    "print(f\"Coupon Redemptions: {coupon_redempt.count():,} rows\")\n",
    "print(f\"Transaction Data: {transaction_data.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf90248-ed56-4e81-8d36-0e6e71c385d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA QUALITY VALIDATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clean transaction data - remove anomalies\n",
    "initial_count = transaction_data.count()\n",
    "\n",
    "transaction_data = transaction_data.filter(\n",
    "    (col(\"QUANTITY\") != 0) &\n",
    "    (col(\"RETAIL_DISC\") <= 0)\n",
    ")\n",
    "\n",
    "cleaned_count = transaction_data.count()\n",
    "removed = initial_count - cleaned_count\n",
    "\n",
    "print(f\"Transactions cleaned: {removed:,} invalid records removed\")\n",
    "print(f\"Valid transactions: {cleaned_count:,} rows\")\n",
    "\n",
    "# Validate campaign dates\n",
    "invalid_dates = campaign_desc.filter(col(\"START_DAY\") > col(\"END_DAY\")).count()\n",
    "if invalid_dates == 0:\n",
    "    print(\"Campaign dates validated: All START_DAY <= END_DAY\")\n",
    "else:\n",
    "    print(f\"Warning: {invalid_dates} campaigns with invalid date ranges\")\n",
    "\n",
    "print(\"\\n Data quality validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23025267-1f1b-4d2b-914c-8f3c7a7c5006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cbd059b-899d-4a48-ac3d-62feab1fb530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CAMPAIGN ENRICHMENT\n",
    "# ============================================================\n",
    "\n",
    "print(\"Building campaign enrichment...\")\n",
    "\n",
    "# Combine campaign table with campaign descriptions\n",
    "combined_campaign = (\n",
    "    campaign_table.alias(\"t\")\n",
    "    .join(\n",
    "        campaign_desc.select(\"CAMPAIGN\", \"START_DAY\", \"END_DAY\").alias(\"d\"),\n",
    "        on=\"CAMPAIGN\",\n",
    "        how=\"left\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Combined campaign data: {combined_campaign.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fc4877e-5a9c-4089-84d2-397f50cb8419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRANSACTION AGGREGATION - DAILY HOUSEHOLD LEVEL\n",
    "# ============================================================\n",
    "\n",
    "print(\"Aggregating transactions to daily household level...\")\n",
    "\n",
    "t = transaction_data.alias(\"t\")\n",
    "c = combined_campaign.alias(\"c\")\n",
    "\n",
    "# Join transactions with campaigns\n",
    "joined = (\n",
    "    t.join(\n",
    "        c,\n",
    "        (col(\"t.household_key\") == col(\"c.household_key\")) &\n",
    "        (col(\"t.DAY\") >= col(\"c.START_DAY\")) &\n",
    "        (col(\"t.DAY\") <= col(\"c.END_DAY\")),\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"t.household_key\").alias(\"household_key\"),\n",
    "        col(\"t.DAY\").alias(\"DAY\"),\n",
    "        col(\"t.QUANTITY\").alias(\"QUANTITY\"),\n",
    "        col(\"t.SALES_VALUE\").alias(\"SALES_VALUE\"),\n",
    "        col(\"t.RETAIL_DISC\").alias(\"RETAIL_DISC\"),\n",
    "        col(\"t.COUPON_DISC\").alias(\"COUPON_DISC\"),\n",
    "        col(\"c.CAMPAIGN\").alias(\"CAMPAIGN\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Aggregate to daily household level with campaign flag\n",
    "transaction_daily = (\n",
    "    joined\n",
    "    .groupBy(\"household_key\", \"DAY\")\n",
    "    .agg(\n",
    "        first(\"QUANTITY\").alias(\"QUANTITY\"),\n",
    "        first(\"SALES_VALUE\").alias(\"SALES_VALUE\"),\n",
    "        first(\"RETAIL_DISC\").alias(\"RETAIL_DISC\"),\n",
    "        first(\"COUPON_DISC\").alias(\"COUPON_DISC\"),\n",
    "        max(when(col(\"CAMPAIGN\").isNotNull(), 1).otherwise(0)).alias(\"campaign_flag\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Daily household transactions: {transaction_daily.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e4d6011-970f-42fd-b427-58f8bd3a585c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ADD COUPON REDEMPTION FLAGS\n",
    "# ============================================================\n",
    "\n",
    "print(\"Adding coupon redemption flags...\")\n",
    "\n",
    "transaction_daily = (\n",
    "    transaction_daily.alias(\"t\")\n",
    "    .join(\n",
    "        coupon_redempt\n",
    "            .select(\"household_key\", \"DAY\")\n",
    "            .dropDuplicates()\n",
    "            .withColumn(\"coupon_redempt_flag\", lit(1))\n",
    "            .alias(\"c\"),\n",
    "        on=[\"household_key\", \"DAY\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"coupon_redempt_flag\",\n",
    "        coalesce(col(\"coupon_redempt_flag\"), lit(0))\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Transaction daily with coupon flags: {transaction_daily.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a67a04d8-8bbe-4900-b7d4-43e878125f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BUSINESS DAILY AGGREGATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"Aggregating to business daily metrics...\")\n",
    "\n",
    "business_daily = (\n",
    "    transaction_daily\n",
    "    .groupBy(\"DAY\")\n",
    "    .agg(\n",
    "        round(sum(\"SALES_VALUE\"), 2).alias(\"total_sales\"),\n",
    "        round(sum(\"RETAIL_DISC\"), 2).alias(\"total_retail_disc\"),\n",
    "        round(sum(\"COUPON_DISC\"), 2).alias(\"total_coupon_disc\"),\n",
    "        countDistinct(\n",
    "            when(col(\"campaign_flag\") == 1, col(\"household_key\"))\n",
    "        ).alias(\"campaign_count\"),\n",
    "        countDistinct(\n",
    "            when(col(\"coupon_redempt_flag\") == 1, col(\"household_key\"))\n",
    "        ).alias(\"coupon_redempt_count\"),\n",
    "    )\n",
    "    .fillna(0)\n",
    "    .orderBy(\"DAY\")\n",
    ")\n",
    "\n",
    "print(f\"Business daily metrics: {business_daily.count():,} days\")\n",
    "print(\"\\n Feature engineering complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e3fa359-440a-4619-9c4c-6af55abf32a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 3. Anomaly Detection - Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "266cbd3d-82e0-4cf1-87bd-c9e52d9afeb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PROPHET MODEL - SALES ANOMALY DETECTION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SALES ANOMALY DETECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare data for Prophet\n",
    "sales_pd = business_daily.select(\"DAY\", \"total_sales\").orderBy(\"DAY\").toPandas()\n",
    "sales_pd[\"ds\"] = pd.to_datetime(sales_pd[\"DAY\"], unit=\"D\", origin=\"1960-01-01\")\n",
    "sales_pd[\"y\"] = sales_pd[\"total_sales\"]\n",
    "sales_pd = sales_pd[[\"ds\", \"y\"]]\n",
    "\n",
    "print(f\"Training Prophet model on {len(sales_pd)} days of sales data...\")\n",
    "\n",
    "# Train Prophet model\n",
    "m_sales = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,\n",
    "    interval_width=0.95\n",
    ")\n",
    "m_sales.fit(sales_pd)\n",
    "\n",
    "# Generate forecast\n",
    "future_sales = m_sales.make_future_dataframe(periods=30)\n",
    "forecast_sales = m_sales.predict(future_sales)\n",
    "\n",
    "# Detect anomalies\n",
    "merged_sales = sales_pd.merge(\n",
    "    forecast_sales[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "    on=\"ds\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "merged_sales[\"is_anomaly\"] = (\n",
    "    (merged_sales[\"y\"] > merged_sales[\"yhat_upper\"]) | \n",
    "    (merged_sales[\"y\"] < merged_sales[\"yhat_lower\"])\n",
    ")\n",
    "\n",
    "# Separate upper and lower bound anomalies\n",
    "merged_sales[\"anomaly_upper\"] = merged_sales[\"y\"] > merged_sales[\"yhat_upper\"]\n",
    "merged_sales[\"anomaly_lower\"] = merged_sales[\"y\"] < merged_sales[\"yhat_lower\"]\n",
    "\n",
    "# Count anomalies\n",
    "num_sales_anomalies = merged_sales[\"is_anomaly\"].sum()\n",
    "num_sales_upper = merged_sales[\"anomaly_upper\"].sum()\n",
    "num_sales_lower = merged_sales[\"anomaly_lower\"].sum()\n",
    "pct_sales_anomalies = (num_sales_anomalies / len(merged_sales)) * 100\n",
    "\n",
    "print(f\"\\n Sales anomalies detected: {num_sales_anomalies} days ({pct_sales_anomalies:.2f}%)\")\n",
    "print(f\"  - Above upper bound: {num_sales_upper} days\")\n",
    "print(f\"  - Below lower bound: {num_sales_lower} days\")\n",
    "\n",
    "# Store results\n",
    "anomaly_sales_pd = merged_sales[merged_sales[\"is_anomaly\"] == True][[\"ds\", \"y\", \"yhat_upper\", \"yhat_lower\", \"anomaly_upper\", \"anomaly_lower\"]].copy()\n",
    "anomaly_sales_pd[\"DAY\"] = (anomaly_sales_pd[\"ds\"] - pd.Timestamp(\"1960-01-01\")).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d69638-093e-4ad9-8007-d6038a46daf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 4. Anomaly Detection - Retail Discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64a2731d-bdcb-40be-9b06-fb2e09fde8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PROPHET MODEL - RETAIL DISCOUNT ANOMALY DETECTION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETAIL DISCOUNT ANOMALY DETECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare data\n",
    "retail_pd = (\n",
    "    business_daily\n",
    "    .select(\"DAY\", \"total_retail_disc\", \"campaign_count\")\n",
    "    .orderBy(\"DAY\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "retail_pd[\"ds\"] = pd.to_datetime(retail_pd[\"DAY\"], unit=\"D\", origin=\"1960-01-01\")\n",
    "retail_pd[\"y\"] = retail_pd[\"total_retail_disc\"].abs()\n",
    "retail_pd[\"campaign_count\"] = retail_pd[\"campaign_count\"]\n",
    "retail_pd = retail_pd[[\"ds\", \"y\", \"campaign_count\"]]\n",
    "\n",
    "print(f\"Training Prophet model on {len(retail_pd)} days of retail discount data...\")\n",
    "\n",
    "# Train model with campaign_count as regressor\n",
    "m_retail = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,\n",
    "    interval_width=0.95\n",
    ")\n",
    "m_retail.add_regressor(\"campaign_count\")\n",
    "m_retail.fit(retail_pd)\n",
    "\n",
    "# Forecast\n",
    "future_retail = m_retail.make_future_dataframe(periods=30)\n",
    "last_campaign = retail_pd[\"campaign_count\"].iloc[-1]\n",
    "future_retail[\"campaign_count\"] = last_campaign\n",
    "forecast_retail = m_retail.predict(future_retail)\n",
    "\n",
    "# Detect anomalies\n",
    "merged_retail = retail_pd.merge(\n",
    "    forecast_retail[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "    on=\"ds\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "merged_retail[\"is_anomaly\"] = (\n",
    "    (merged_retail[\"y\"] > merged_retail[\"yhat_upper\"]) |\n",
    "    (merged_retail[\"y\"] < merged_retail[\"yhat_lower\"])\n",
    ")\n",
    "\n",
    "# Separate upper and lower bound anomalies\n",
    "merged_retail[\"anomaly_upper\"] = merged_retail[\"y\"] > merged_retail[\"yhat_upper\"]\n",
    "merged_retail[\"anomaly_lower\"] = merged_retail[\"y\"] < merged_retail[\"yhat_lower\"]\n",
    "\n",
    "# Count anomalies\n",
    "num_retail_anomalies = merged_retail[\"is_anomaly\"].sum()\n",
    "num_retail_upper = merged_retail[\"anomaly_upper\"].sum()\n",
    "num_retail_lower = merged_retail[\"anomaly_lower\"].sum()\n",
    "pct_retail_anomalies = (num_retail_anomalies / len(merged_retail)) * 100\n",
    "\n",
    "print(f\"\\n Retail discount anomalies detected: {num_retail_anomalies} days ({pct_retail_anomalies:.2f}%)\")\n",
    "print(f\"  - Above upper bound: {num_retail_upper} days\")\n",
    "print(f\"  - Below lower bound: {num_retail_lower} days\")\n",
    "\n",
    "# Store results\n",
    "retail_anom_pd = merged_retail[merged_retail[\"is_anomaly\"] == True][[\"ds\", \"y\", \"yhat_upper\", \"yhat_lower\", \"anomaly_upper\", \"anomaly_lower\"]].copy()\n",
    "retail_anom_pd[\"DAY\"] = (retail_anom_pd[\"ds\"] - pd.Timestamp(\"1960-01-01\")).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c472c6d-1710-47a5-adf0-231a825e886e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 5. Anomaly Detection - Coupon Discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9ab0986-3479-431d-ab76-c2d2151d262a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PROPHET MODEL - COUPON DISCOUNT ANOMALY DETECTION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COUPON DISCOUNT ANOMALY DETECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare data\n",
    "coupon_pd = (\n",
    "    business_daily\n",
    "    .select(\"DAY\", \"total_coupon_disc\", \"campaign_count\", \"coupon_redempt_count\")\n",
    "    .orderBy(\"DAY\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "coupon_pd[\"ds\"] = pd.to_datetime(coupon_pd[\"DAY\"], unit=\"D\", origin=\"1960-01-01\")\n",
    "coupon_pd[\"y\"] = coupon_pd[\"total_coupon_disc\"].abs()\n",
    "coupon_pd[\"campaign_count\"] = coupon_pd[\"campaign_count\"]\n",
    "coupon_pd[\"coupon_redempt_count\"] = coupon_pd[\"coupon_redempt_count\"]\n",
    "coupon_pd = coupon_pd[[\"ds\", \"y\", \"campaign_count\", \"coupon_redempt_count\"]]\n",
    "\n",
    "print(f\"Training Prophet model on {len(coupon_pd)} days of coupon discount data...\")\n",
    "\n",
    "# Train model with regressors\n",
    "m_coupon = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,\n",
    "    interval_width=0.95\n",
    ")\n",
    "m_coupon.add_regressor(\"campaign_count\")\n",
    "m_coupon.add_regressor(\"coupon_redempt_count\")\n",
    "m_coupon.fit(coupon_pd)\n",
    "\n",
    "# Forecast\n",
    "future_coupon = m_coupon.make_future_dataframe(periods=30)\n",
    "future_coupon[\"campaign_count\"] = coupon_pd[\"campaign_count\"].iloc[-1]\n",
    "future_coupon[\"coupon_redempt_count\"] = coupon_pd[\"coupon_redempt_count\"].iloc[-1]\n",
    "forecast_coupon = m_coupon.predict(future_coupon)\n",
    "\n",
    "# Detect anomalies\n",
    "merged_coupon = coupon_pd.merge(\n",
    "    forecast_coupon[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]],\n",
    "    on=\"ds\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "merged_coupon[\"is_anomaly\"] = (\n",
    "    (merged_coupon[\"y\"] > merged_coupon[\"yhat_upper\"]) |\n",
    "    (merged_coupon[\"y\"] < merged_coupon[\"yhat_lower\"])\n",
    ")\n",
    "\n",
    "# Separate upper and lower bound anomalies\n",
    "merged_coupon[\"anomaly_upper\"] = merged_coupon[\"y\"] > merged_coupon[\"yhat_upper\"]\n",
    "merged_coupon[\"anomaly_lower\"] = merged_coupon[\"y\"] < merged_coupon[\"yhat_lower\"]\n",
    "\n",
    "# Count anomalies\n",
    "num_coupon_anomalies = merged_coupon[\"is_anomaly\"].sum()\n",
    "num_coupon_upper = merged_coupon[\"anomaly_upper\"].sum()\n",
    "num_coupon_lower = merged_coupon[\"anomaly_lower\"].sum()\n",
    "pct_coupon_anomalies = (num_coupon_anomalies / len(merged_coupon)) * 100\n",
    "\n",
    "print(f\"\\n Coupon discount anomalies detected: {num_coupon_anomalies} days ({pct_coupon_anomalies:.2f}%)\")\n",
    "print(f\"  - Above upper bound: {num_coupon_upper} days\")\n",
    "print(f\"  - Below lower bound: {num_coupon_lower} days\")\n",
    "\n",
    "# Store results\n",
    "coupon_anom_pd = merged_coupon[merged_coupon[\"is_anomaly\"] == True][[\"ds\", \"y\", \"yhat_upper\", \"yhat_lower\", \"anomaly_upper\", \"anomaly_lower\"]].copy()\n",
    "coupon_anom_pd[\"DAY\"] = (coupon_anom_pd[\"ds\"] - pd.Timestamp(\"1960-01-01\")).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "def48c7e-2463-4ce9-a1ee-85bfc5cec5eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 6. Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fa8598a-76b2-487d-a35d-694c40b54e47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXECUTIVE SUMMARY - KEY METRICS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXECUTIVE SUMMARY - ANOMALY DETECTION REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate summary statistics\n",
    "total_days = len(sales_pd)\n",
    "date_range_start = sales_pd[\"ds\"].min().strftime(\"%Y-%m-%d\")\n",
    "date_range_end = sales_pd[\"ds\"].max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Sales metrics\n",
    "avg_daily_sales = merged_sales[\"y\"].mean()\n",
    "total_sales = merged_sales[\"y\"].sum()\n",
    "sales_anomaly_days = merged_sales[merged_sales[\"is_anomaly\"] == True]\n",
    "\n",
    "# Retail discount metrics\n",
    "avg_retail_disc = merged_retail[\"y\"].mean()\n",
    "total_retail_disc = merged_retail[\"y\"].sum()\n",
    "retail_anomaly_days = merged_retail[merged_retail[\"is_anomaly\"] == True]\n",
    "\n",
    "# Coupon discount metrics\n",
    "avg_coupon_disc = merged_coupon[\"y\"].mean()\n",
    "total_coupon_disc = merged_coupon[\"y\"].sum()\n",
    "coupon_anomaly_days = merged_coupon[merged_coupon[\"is_anomaly\"] == True]\n",
    "\n",
    "print(f\"\\n ANALYSIS PERIOD\")\n",
    "print(f\"   Date Range: {date_range_start} to {date_range_end}\")\n",
    "print(f\"   Total Days Analyzed: {total_days:,}\")\n",
    "\n",
    "print(f\"\\n SALES METRICS\")\n",
    "print(f\"   Total Sales: ${total_sales:,.2f}\")\n",
    "print(f\"   Average Daily Sales: ${avg_daily_sales:,.2f}\")\n",
    "print(f\"   Anomaly Days: {num_sales_anomalies} ({pct_sales_anomalies:.2f}%)\")\n",
    "print(f\"      - Above upper bound: {num_sales_upper} days\")\n",
    "print(f\"      - Below lower bound: {num_sales_lower} days\")\n",
    "if num_sales_anomalies > 0:\n",
    "    max_sales_anomaly = sales_anomaly_days.loc[sales_anomaly_days[\"y\"].idxmax()]\n",
    "    min_sales_anomaly = sales_anomaly_days.loc[sales_anomaly_days[\"y\"].idxmin()]\n",
    "    print(f\"   Highest Anomaly: ${max_sales_anomaly['y']:,.2f} on {max_sales_anomaly['ds'].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Lowest Anomaly: ${min_sales_anomaly['y']:,.2f} on {min_sales_anomaly['ds'].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\n RETAIL DISCOUNT METRICS\")\n",
    "print(f\"   Total Retail Discounts: ${total_retail_disc:,.2f}\")\n",
    "print(f\"   Average Daily Discount: ${avg_retail_disc:,.2f}\")\n",
    "print(f\"   Anomaly Days: {num_retail_anomalies} ({pct_retail_anomalies:.2f}%)\")\n",
    "print(f\"      - Above upper bound: {num_retail_upper} days\")\n",
    "print(f\"      - Below lower bound: {num_retail_lower} days\")\n",
    "if num_retail_anomalies > 0:\n",
    "    max_retail_anomaly = retail_anomaly_days.loc[retail_anomaly_days[\"y\"].idxmax()]\n",
    "    print(f\"   Highest Anomaly: ${max_retail_anomaly['y']:,.2f} on {max_retail_anomaly['ds'].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\n COUPON DISCOUNT METRICS\")\n",
    "print(f\"   Total Coupon Discounts: ${total_coupon_disc:,.2f}\")\n",
    "print(f\"   Average Daily Discount: ${avg_coupon_disc:,.2f}\")\n",
    "print(f\"   Anomaly Days: {num_coupon_anomalies} ({pct_coupon_anomalies:.2f}%)\")\n",
    "print(f\"      - Above upper bound: {num_coupon_upper} days\")\n",
    "print(f\"      - Below lower bound: {num_coupon_lower} days\")\n",
    "if num_coupon_anomalies > 0:\n",
    "    max_coupon_anomaly = coupon_anomaly_days.loc[coupon_anomaly_days[\"y\"].idxmax()]\n",
    "    print(f\"   Highest Anomaly: ${max_coupon_anomaly['y']:,.2f} on {max_coupon_anomaly['ds'].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Executive Summary Complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecd471a2-2156-49a8-a41e-9546db3f7684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 7. Visualizations - Sales Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b3817d-b08f-47af-b4fc-5688c0b52476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 1: SALES FORECAST WITH ANOMALIES\n",
    "# ============================================================\n",
    "\n",
    "fig_sales = plot_plotly(m_sales, forecast_sales)\n",
    "fig_sales.update_layout(\n",
    "    title=\"Daily Sales Forecast with Confidence Intervals\",\n",
    "    height=600,\n",
    "    showlegend=True\n",
    ")\n",
    "fig_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3daa56da-4521-4ad9-8c2b-252505582db1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 2: SALES ANOMALIES HIGHLIGHTED\n",
    "# ============================================================\n",
    "\n",
    "fig_sales_scatter = px.scatter(\n",
    "    merged_sales,\n",
    "    x=\"ds\",\n",
    "    y=\"y\",\n",
    "    color=\"is_anomaly\",\n",
    "    color_discrete_map={False: \"blue\", True: \"red\"},\n",
    "    title=\"Sales Anomalies - Red Points Indicate Outliers\",\n",
    "    labels={\"ds\": \"Date\", \"y\": \"Total Daily Sales ($)\"},\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig_sales_scatter.add_trace(\n",
    "    go.Scatter(\n",
    "        x=merged_sales[\"ds\"],\n",
    "        y=merged_sales[\"yhat_upper\"],\n",
    "        mode=\"lines\",\n",
    "        line=dict(dash=\"dash\", color=\"gray\"),\n",
    "        name=\"Upper Bound\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_sales_scatter.add_trace(\n",
    "    go.Scatter(\n",
    "        x=merged_sales[\"ds\"],\n",
    "        y=merged_sales[\"yhat_lower\"],\n",
    "        mode=\"lines\",\n",
    "        line=dict(dash=\"dash\", color=\"gray\"),\n",
    "        name=\"Lower Bound\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_sales_scatter.update_layout(\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Sales ($)\",\n",
    "    hovermode=\"x unified\"\n",
    ")\n",
    "\n",
    "fig_sales_scatter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c311bebe-7f96-4658-929e-89ebcc4b37d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 3: SALES TREND COMPONENTS\n",
    "# ============================================================\n",
    "\n",
    "fig_sales_components = plot_components_plotly(m_sales, forecast_sales)\n",
    "fig_sales_components.update_layout(\n",
    "    title=\"Sales Trend Decomposition - Yearly & Weekly Patterns\",\n",
    "    height=600\n",
    ")\n",
    "fig_sales_components.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef2f445-5916-41c4-b350-06aa90e098c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 4: CAMPAIGNS AND REDEMPTIONS PER DAY\n",
    "# ============================================================\n",
    "\n",
    "# Prepare data\n",
    "campaigns_redemptions_pd = business_daily.select(\"DAY\", \"campaign_count\", \"coupon_redempt_count\").orderBy(\"DAY\").toPandas()\n",
    "campaigns_redemptions_pd[\"ds\"] = pd.to_datetime(campaigns_redemptions_pd[\"DAY\"], unit=\"D\", origin=\"1960-01-01\")\n",
    "\n",
    "# Create dual-axis plot\n",
    "fig_campaigns = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=(\"Daily Campaign Participation\", \"Daily Coupon Redemptions\"),\n",
    "    vertical_spacing=0.12\n",
    ")\n",
    "\n",
    "# Campaign count\n",
    "fig_campaigns.add_trace(\n",
    "    go.Scatter(\n",
    "        x=campaigns_redemptions_pd[\"ds\"],\n",
    "        y=campaigns_redemptions_pd[\"campaign_count\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Campaign Count\",\n",
    "        line=dict(color=\"blue\", width=1.5),\n",
    "        fill=\"tozeroy\",\n",
    "        fillcolor=\"rgba(0, 100, 255, 0.2)\"\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Coupon redemptions\n",
    "fig_campaigns.add_trace(\n",
    "    go.Scatter(\n",
    "        x=campaigns_redemptions_pd[\"ds\"],\n",
    "        y=campaigns_redemptions_pd[\"coupon_redempt_count\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"Redemption Count\",\n",
    "        line=dict(color=\"purple\", width=1.5),\n",
    "        fill=\"tozeroy\",\n",
    "        fillcolor=\"rgba(128, 0, 128, 0.2)\"\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig_campaigns.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "fig_campaigns.update_yaxes(title_text=\"# of Households\", row=1, col=1)\n",
    "fig_campaigns.update_yaxes(title_text=\"# of Households\", row=2, col=1)\n",
    "\n",
    "fig_campaigns.update_layout(\n",
    "    title_text=\"Campaign Participation & Coupon Redemption Trends\",\n",
    "    height=700,\n",
    "    showlegend=True,\n",
    "    hovermode=\"x unified\"\n",
    ")\n",
    "\n",
    "fig_campaigns.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de2cf51-7710-48c4-b1a6-6c03ad968e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 8. Visualizations - Retail Discount Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9476db1-eb3d-4391-a3f5-49ae69eaf8f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 4: RETAIL DISCOUNT FORECAST\n",
    "# ============================================================\n",
    "\n",
    "fig_retail = plot_plotly(m_retail, forecast_retail)\n",
    "fig_retail.update_layout(\n",
    "    title=\"Daily Retail Discount Forecast with Confidence Intervals\",\n",
    "    height=600\n",
    ")\n",
    "fig_retail.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4729eff3-5d00-425c-8dd2-dfdf8cc63234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 5: RETAIL DISCOUNT ANOMALIES HIGHLIGHTED\n",
    "# ============================================================\n",
    "\n",
    "fig_retail_scatter = px.scatter(\n",
    "    merged_retail,\n",
    "    x=\"ds\",\n",
    "    y=\"y\",\n",
    "    color=\"is_anomaly\",\n",
    "    color_discrete_map={False: \"green\", True: \"red\"},\n",
    "    title=\"Retail Discount Anomalies - Red Points Indicate Outliers\",\n",
    "    labels={\"ds\": \"Date\", \"y\": \"Daily Retail Discount ($)\"},\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig_retail_scatter.add_trace(\n",
    "    go.Scatter(\n",
    "        x=merged_retail[\"ds\"],\n",
    "        y=merged_retail[\"yhat_upper\"],\n",
    "        mode=\"lines\",\n",
    "        line=dict(dash=\"dash\", color=\"gray\"),\n",
    "        name=\"Upper Bound\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_retail_scatter.add_trace(\n",
    "    go.Scatter(\n",
    "        x=merged_retail[\"ds\"],\n",
    "        y=merged_retail[\"yhat_lower\"],\n",
    "        mode=\"lines\",\n",
    "        line=dict(dash=\"dash\", color=\"gray\"),\n",
    "        name=\"Lower Bound\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_retail_scatter.update_layout(\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Retail Discount ($)\",\n",
    "    hovermode=\"x unified\"\n",
    ")\n",
    "\n",
    "fig_retail_scatter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "179510d9-35c9-47e9-8ba3-d768fcee7d17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 9. Visualizations - Coupon Discount Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e22645d-134c-4664-8fa9-628f76503a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 6: COUPON DISCOUNT FORECAST\n",
    "# ============================================================\n",
    "\n",
    "fig_coupon = plot_plotly(m_coupon, forecast_coupon)\n",
    "fig_coupon.update_layout(\n",
    "    title=\"Daily Coupon Discount Forecast with Confidence Intervals\",\n",
    "    height=600\n",
    ")\n",
    "fig_coupon.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ef0c116-6ebb-4b97-b4d9-6005ef30b719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 7: COUPON DISCOUNT ANOMALIES HIGHLIGHTED\n",
    "# ============================================================\n",
    "\n",
    "fig_coupon_scatter = px.scatter(\n",
    "    merged_coupon,\n",
    "    x=\"ds\",\n",
    "    y=\"y\",\n",
    "    color=\"is_anomaly\",\n",
    "    color_discrete_map={False: \"purple\", True: \"red\"},\n",
    "    title=\"Coupon Discount Anomalies - Red Points Indicate Outliers\",\n",
    "    labels={\"ds\": \"Date\", \"y\": \"Daily Coupon Discount ($)\"},\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig_coupon_scatter.add_trace(\n",
    "    go.Scatter(\n",
    "        x=merged_coupon[\"ds\"],\n",
    "        y=merged_coupon[\"yhat_upper\"],\n",
    "        mode=\"lines\",\n",
    "        line=dict(dash=\"dash\", color=\"gray\"),\n",
    "        name=\"Upper Bound\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_coupon_scatter.add_trace(\n",
    "    go.Scatter(\n",
    "        x=merged_coupon[\"ds\"],\n",
    "        y=merged_coupon[\"yhat_lower\"],\n",
    "        mode=\"lines\",\n",
    "        line=dict(dash=\"dash\", color=\"gray\"),\n",
    "        name=\"Lower Bound\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_coupon_scatter.update_layout(\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Coupon Discount ($)\",\n",
    "    hovermode=\"x unified\"\n",
    ")\n",
    "\n",
    "fig_coupon_scatter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f627fb00-7885-4ce0-89f6-271e80198618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 10. Combined Anomaly Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2a9db4c-5bb3-4a8e-8bc0-19384fb4f47a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 8: COMBINED ANOMALY HEATMAP\n",
    "# ============================================================\n",
    "\n",
    "# Create combined anomaly dataset\n",
    "all_dates = pd.DataFrame({\"ds\": merged_sales[\"ds\"]})\n",
    "\n",
    "all_dates[\"sales_anomaly\"] = merged_sales[\"is_anomaly\"].astype(int)\n",
    "all_dates[\"retail_anomaly\"] = merged_retail[\"is_anomaly\"].astype(int)\n",
    "all_dates[\"coupon_anomaly\"] = merged_coupon[\"is_anomaly\"].astype(int)\n",
    "all_dates[\"total_anomalies\"] = all_dates[\"sales_anomaly\"] + all_dates[\"retail_anomaly\"] + all_dates[\"coupon_anomaly\"]\n",
    "\n",
    "# Multi-anomaly days (days with 2+ anomalies)\n",
    "multi_anomaly_days = all_dates[all_dates[\"total_anomalies\"] >= 2]\n",
    "\n",
    "print(f\"\\n COMBINED ANOMALY INSIGHTS\")\n",
    "print(f\"   Days with multiple anomalies: {len(multi_anomaly_days)}\")\n",
    "if len(multi_anomaly_days) > 0:\n",
    "    print(f\"   Dates with multiple anomalies:\")\n",
    "    for _, row in multi_anomaly_days.head(10).iterrows():\n",
    "        anomaly_types = []\n",
    "        if row[\"sales_anomaly\"]: anomaly_types.append(\"Sales\")\n",
    "        if row[\"retail_anomaly\"]: anomaly_types.append(\"Retail\")\n",
    "        if row[\"coupon_anomaly\"]: anomaly_types.append(\"Coupon\")\n",
    "        print(f\"      {row['ds'].strftime('%Y-%m-%d')}: {', '.join(anomaly_types)}\")\n",
    "\n",
    "# Heatmap visualization\n",
    "fig_heatmap = go.Figure()\n",
    "\n",
    "fig_heatmap.add_trace(go.Scatter(\n",
    "    x=all_dates[\"ds\"],\n",
    "    y=[1]*len(all_dates),\n",
    "    mode=\"markers\",\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color=all_dates[\"sales_anomaly\"],\n",
    "        colorscale=[[0, \"lightgray\"], [1, \"red\"]],\n",
    "        showscale=False\n",
    "    ),\n",
    "    name=\"Sales Anomalies\",\n",
    "    hovertemplate=\"Date: %{x}<br>Sales Anomaly: %{marker.color}<extra></extra>\"\n",
    "))\n",
    "\n",
    "fig_heatmap.add_trace(go.Scatter(\n",
    "    x=all_dates[\"ds\"],\n",
    "    y=[2]*len(all_dates),\n",
    "    mode=\"markers\",\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color=all_dates[\"retail_anomaly\"],\n",
    "        colorscale=[[0, \"lightgray\"], [1, \"red\"]],\n",
    "        showscale=False\n",
    "    ),\n",
    "    name=\"Retail Anomalies\",\n",
    "    hovertemplate=\"Date: %{x}<br>Retail Anomaly: %{marker.color}<extra></extra>\"\n",
    "))\n",
    "\n",
    "fig_heatmap.add_trace(go.Scatter(\n",
    "    x=all_dates[\"ds\"],\n",
    "    y=[3]*len(all_dates),\n",
    "    mode=\"markers\",\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color=all_dates[\"coupon_anomaly\"],\n",
    "        colorscale=[[0, \"lightgray\"], [1, \"red\"]],\n",
    "        showscale=False\n",
    "    ),\n",
    "    name=\"Coupon Anomalies\",\n",
    "    hovertemplate=\"Date: %{x}<br>Coupon Anomaly: %{marker.color}<extra></extra>\"\n",
    "))\n",
    "\n",
    "fig_heatmap.update_layout(\n",
    "    title=\"Anomaly Timeline - All Metrics (Red = Anomaly Detected)\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis=dict(\n",
    "        tickvals=[1, 2, 3],\n",
    "        ticktext=[\"Sales\", \"Retail Discount\", \"Coupon Discount\"]\n",
    "    ),\n",
    "    height=400,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig_heatmap.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "606b0f5b-5136-409e-a2fa-5ae5a9a3a385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 9: ANOMALY SUMMARY DASHBOARD\n",
    "# ============================================================\n",
    "\n",
    "# Create summary bar chart\n",
    "summary_data = pd.DataFrame({\n",
    "    \"Metric\": [\"Sales\", \"Retail Discount\", \"Coupon Discount\"],\n",
    "    \"Anomaly Count\": [num_sales_anomalies, num_retail_anomalies, num_coupon_anomalies],\n",
    "    \"Anomaly %\": [pct_sales_anomalies, pct_retail_anomalies, pct_coupon_anomalies]\n",
    "})\n",
    "\n",
    "fig_summary = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"Anomaly Count by Metric\", \"Anomaly Percentage by Metric\"),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "fig_summary.add_trace(\n",
    "    go.Bar(\n",
    "        x=summary_data[\"Metric\"],\n",
    "        y=summary_data[\"Anomaly Count\"],\n",
    "        marker_color=[\"#1f77b4\", \"#2ca02c\", \"#9467bd\"],\n",
    "        text=summary_data[\"Anomaly Count\"],\n",
    "        textposition=\"auto\"\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig_summary.add_trace(\n",
    "    go.Bar(\n",
    "        x=summary_data[\"Metric\"],\n",
    "        y=summary_data[\"Anomaly %\"],\n",
    "        marker_color=[\"#1f77b4\", \"#2ca02c\", \"#9467bd\"],\n",
    "        text=[f\"{x:.2f}%\" for x in summary_data[\"Anomaly %\"]],\n",
    "        textposition=\"auto\"\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig_summary.update_xaxes(title_text=\"Metric\", row=1, col=1)\n",
    "fig_summary.update_xaxes(title_text=\"Metric\", row=1, col=2)\n",
    "fig_summary.update_yaxes(title_text=\"Number of Days\", row=1, col=1)\n",
    "fig_summary.update_yaxes(title_text=\"Percentage (%)\", row=1, col=2)\n",
    "\n",
    "fig_summary.update_layout(\n",
    "    title_text=\"Anomaly Detection Summary Dashboard\",\n",
    "    showlegend=False,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f27c291-7a60-4d4c-a0c2-cd24d312557b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION 10: UPPER vs LOWER BOUND ANOMALY BREAKDOWN\n",
    "# ============================================================\n",
    "\n",
    "# Create detailed breakdown data\n",
    "breakdown_data = pd.DataFrame({\n",
    "    \"Metric\": [\"Sales\", \"Sales\", \"Retail Disc.\", \"Retail Disc.\", \"Coupon Disc.\", \"Coupon Disc.\"],\n",
    "    \"Type\": [\"Above Upper\", \"Below Lower\", \"Above Upper\", \"Below Lower\", \"Above Upper\", \"Below Lower\"],\n",
    "    \"Count\": [num_sales_upper, num_sales_lower, num_retail_upper, num_retail_lower, num_coupon_upper, num_coupon_lower]\n",
    "})\n",
    "\n",
    "fig_breakdown = px.bar(\n",
    "    breakdown_data,\n",
    "    x=\"Metric\",\n",
    "    y=\"Count\",\n",
    "    color=\"Type\",\n",
    "    barmode=\"group\",\n",
    "    title=\"Anomaly Breakdown: Upper vs Lower Bound Violations\",\n",
    "    labels={\"Count\": \"Number of Days\", \"Metric\": \"Metric Type\"},\n",
    "    color_discrete_map={\"Above Upper\": \"#FF6B6B\", \"Below Lower\": \"#4ECDC4\"},\n",
    "    text=\"Count\",\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig_breakdown.update_traces(textposition=\"outside\")\n",
    "fig_breakdown.update_layout(\n",
    "    showlegend=True,\n",
    "    legend_title=\"Anomaly Type\",\n",
    "    xaxis_title=\"Metric\",\n",
    "    yaxis_title=\"Number of Anomalous Days\"\n",
    ")\n",
    "\n",
    "fig_breakdown.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UPPER vs LOWER BOUND ANOMALY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n SALES:\")\n",
    "print(f\"   Above upper bound: {num_sales_upper} days ({(num_sales_upper/len(merged_sales)*100):.2f}%)\")\n",
    "print(f\"   Below lower bound: {num_sales_lower} days ({(num_sales_lower/len(merged_sales)*100):.2f}%)\")\n",
    "\n",
    "print(f\"\\n RETAIL DISCOUNT:\")\n",
    "print(f\"   Above upper bound: {num_retail_upper} days ({(num_retail_upper/len(merged_retail)*100):.2f}%)\")\n",
    "print(f\"   Below lower bound: {num_retail_lower} days ({(num_retail_lower/len(merged_retail)*100):.2f}%)\")\n",
    "\n",
    "print(f\"\\n COUPON DISCOUNT:\")\n",
    "print(f\"   Above upper bound: {num_coupon_upper} days ({(num_coupon_upper/len(merged_coupon)*100):.2f}%)\")\n",
    "print(f\"   Below lower bound: {num_coupon_lower} days ({(num_coupon_lower/len(merged_coupon)*100):.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9a05dd-cfb5-4fd4-b44d-f188090a8f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## 11. Anomaly Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f4d27e-b611-49e7-958b-354f75e697cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPORT ANOMALY DETAILS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANOMALY DETAILS - EXPORTABLE DATAFRAMES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sales anomalies\n",
    "if len(anomaly_sales_pd) > 0:\n",
    "    print(f\"\\n SALES ANOMALIES ({len(anomaly_sales_pd)} days)\")\n",
    "    print(f\"   - Above upper bound: {anomaly_sales_pd['anomaly_upper'].sum()} days\")\n",
    "    print(f\"   - Below lower bound: {anomaly_sales_pd['anomaly_lower'].sum()} days\")\n",
    "    print(\"\\n   Details:\")\n",
    "    display_sales = anomaly_sales_pd.copy()\n",
    "    display_sales[\"anomaly_type\"] = display_sales.apply(\n",
    "        lambda row: \"Above Upper\" if row[\"anomaly_upper\"] else \"Below Lower\", axis=1\n",
    "    )\n",
    "    print(display_sales[[\"ds\", \"y\", \"yhat_upper\", \"yhat_lower\", \"anomaly_type\"]].to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n SALES ANOMALIES: None detected\")\n",
    "\n",
    "# Retail discount anomalies\n",
    "if len(retail_anom_pd) > 0:\n",
    "    print(f\"\\n RETAIL DISCOUNT ANOMALIES ({len(retail_anom_pd)} days)\")\n",
    "    print(f\"   - Above upper bound: {retail_anom_pd['anomaly_upper'].sum()} days\")\n",
    "    print(f\"   - Below lower bound: {retail_anom_pd['anomaly_lower'].sum()} days\")\n",
    "    print(\"\\n   Details:\")\n",
    "    display_retail = retail_anom_pd.copy()\n",
    "    display_retail[\"anomaly_type\"] = display_retail.apply(\n",
    "        lambda row: \"Above Upper\" if row[\"anomaly_upper\"] else \"Below Lower\", axis=1\n",
    "    )\n",
    "    print(display_retail[[\"ds\", \"y\", \"yhat_upper\", \"yhat_lower\", \"anomaly_type\"]].to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n RETAIL DISCOUNT ANOMALIES: None detected\")\n",
    "\n",
    "# Coupon discount anomalies\n",
    "if len(coupon_anom_pd) > 0:\n",
    "    print(f\"\\n COUPON DISCOUNT ANOMALIES ({len(coupon_anom_pd)} days)\")\n",
    "    print(f\"   - Above upper bound: {coupon_anom_pd['anomaly_upper'].sum()} days\")\n",
    "    print(f\"   - Below lower bound: {coupon_anom_pd['anomaly_lower'].sum()} days\")\n",
    "    print(\"\\n   Details:\")\n",
    "    display_coupon = coupon_anom_pd.copy()\n",
    "    display_coupon[\"anomaly_type\"] = display_coupon.apply(\n",
    "        lambda row: \"Above Upper\" if row[\"anomaly_upper\"] else \"Below Lower\", axis=1\n",
    "    )\n",
    "    print(display_coupon[[\"ds\", \"y\", \"yhat_upper\", \"yhat_lower\", \"anomaly_type\"]].to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n COUPON DISCOUNT ANOMALIES: None detected\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Anomaly Detection Report Complete\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert to Spark DataFrames for database storage\n",
    "# anomaly_sales_spark = spark.createDataFrame(anomaly_sales_pd[[\"DAY\", \"y\", \"yhat_upper\", \"yhat_lower\"]])\n",
    "# retail_anom_spark = spark.createDataFrame(retail_anom_pd[[\"DAY\", \"y\", \"yhat_upper\", \"yhat_lower\"]])\n",
    "# coupon_anom_spark = spark.createDataFrame(coupon_anom_pd[[\"DAY\", \"y\", \"yhat_upper\", \"yhat_lower\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2df89580-5d6c-47f9-9bea-4cc618824b52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This production notebook has successfully:\n",
    "1. Loaded and validated retail transaction data\n",
    "2. Performed data quality checks and cleaning\n",
    "3. Engineered features for anomaly detection\n",
    "4. Trained Prophet models for Sales, Retail Discount, and Coupon Discount\n",
    "5. Detected anomalies using 95% confidence intervals\n",
    "6. Generated executive summary with key insights\n",
    "7. Created comprehensive visualizations\n",
    "8. Exported anomaly details for further analysis"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Retail_Anomaly_Detection_Production",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
